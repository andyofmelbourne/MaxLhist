\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
%\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%Gummi|065|=)
\title{\textbf{Finding adu distributions or finding photons}}
\author{Andrew Morgan}
\date{}
\usepackage{graphicx}

\begin{document}

\maketitle

\section{Maximum Likelihood of detector counts}
Thought experiment:

\begin{itemize}

  \item We have a histogram dataset $h^m_i$ from $M$ pixels measured over $I$ adu bins.
  \item At each pixel the probability of measuring a given (adu) value is $p^m(x) = f^m(g_m x - \mu_m)$, where $g_m$ is the inverse of the gain and $\mu_m$ is the offset (or dark value) for pixel $m$. 
  \item For all pixels $p$ is composed of $V$ random variables $X^v_i$.
  \item At each pixel the count fraction of each $X^v_i$ is given by $n^m_v$.
\end{itemize}

\textbf{Question}: Can we determine $n^m_v$ and $X^v(x)$ from the set of $x^m_n$ values?

The probability that a single value $x^m_n$ is equal to $x$, given a distribution $p^m(x)$, is given by:
\begin{align}
   Pr(x^m_n = x; p) = p^m(x)
\end{align}

After $N$ trials the probablity of measuring the value $x$ $h^m_x$ times is:
\begin{align}fff
   Pr(h^m_x = k; p) = \frac{N!}{k!(N-k)!} p^m(x)^k \times (1-p^m(x))^{1-k}
\end{align}

After $N$ trials the probablity of measuring the set of values $x^m_n$ $h^m_x$ times is:
\begin{align}
   Pr(x^m_n; p) = \frac{N!}{h^m_0! h^m_1! \cdots h^m_{I-1}!} \prod_{x=0}^{I-1} p^m(x)^{h^m_{x}}
\end{align}


After $M$ sets of $N$ trials the probability of measuring the set of values $x^m_n$ $h^m_x$ times is:
\begin{align}
   Pr(x^m_n; p) = \prod_{m=0}^{M-1}\frac{N!}{h^m_0! h^m_1! \cdots h^m_{I-1}!} \prod_{x=0}^{I-1} p^m(x)^{h^m_{x}}
\end{align}
In principle we should not care about the ordering of $x$ samples between pixels (values of $m$). However when the number of samples for each pixel is large this should not effect the analysis too much. 

The log likelihood is then:
\begin{align}
   \varepsilon(p) &= -\ln(Pr(x^m_n; p)) = -\ln\left(\prod_{m=0}^{M-1}\frac{N!}{h^m_0! h^m_1! \cdots h^m_{I-1}!} \prod_{x=0}^{I-1} p^m(x)^{h^m_{x}}\right) \\
   &= -\sum_{m=0}^{M-1}\ln\left(\frac{N!}{h^m_0! h^m_1! \cdots h^m_{I-1}!} \prod_{x=0}^{I-1} p^m(x)^{h^m_{x}}\right) \\
   &= -\sum_{m=0}^{M-1}\left[\ln\left(\frac{N!}{h^m_0! h^m_1! \cdots h^m_{I-1}!}\right) +\ln\left(\prod_{x=0}^{I-1} p^m(x)^{h^m_{x}}\right) \right]\\
   &= -\sum_{m=0}^{M-1}\left[\ln\left(\frac{N!}{h^m_0! h^m_1! \cdots h^m_{I-1}!}\right) +\sum_{x=0}^{I-1} h^m_x \ln\left(p^m(x)\right) \right]\\
   &= -\sum_{m=0}^{M-1}\ln\left(\frac{N!}{h^m_0! h^m_1! \cdots h^m_{I-1}!}\right) -\sum_{m=0}^{M-1} \sum_{x=0}^{I-1} h^m_x \ln\left(p^m(x)\right) 
   \label{log_like}
\end{align}
The term on the left in Eq. (\ref{log_like}) is the combinatorial factor and is independent of our probability model. Thus, in what follows, we will secretly ignore this constant offset to the error function. This leaves us with:
\begin{align}
   \varepsilon(p) &= -\sum_{m=0}^{M-1} \sum_{x=0}^{I-1} h^m_x \ln\left(p^m(x)\right) 
   \label{log_like}
\end{align}
with a minimum value of 0. 
At each pixel the probability distribution $p^m$ is given by:
\begin{align}
   p^m(x) = f^m(g_m (x - \mu_m)) = \sum_v^V n^m_v X^v(g_m( x - \mu_m))
\end{align}
such that:
\begin{align}
   \sum_v X^v_i &= 1 \quad \text{for all } v \\
   \text{and} \quad \sum_v n^m_v &= 1  \quad \text{for all } m 
\end{align}



















\subsection{Refinement strategy}
At the moment I am thinking that the simplest course of action is to update each of the parameters independently. That is, we update all of the variables in terms of the same set of values from the previous iteration. Now we have a new set of variables that are all "pulling" in different directions. The state in next iteration is then this new set of variables. Also I will call $p^m(x) \rightarrow p^m_i$ where $i$ is the discrete sampling of $x$. 

Our current model has:
\begin{itemize}
  \item $\mu_m$ : the offsets at each pixel.
  \item $g_m$   : the inverse gain at each pixel.
  \item $X^v_i$ : the (adu) distributions for each random variable (i.e. [noise, single photon, flouresence ...])
  \item $n^m_v$ : the count fractions at each pixel for each random variable. The counts $= (\sum_i h^m_i) \times n^m_v = N^m_v$ is probably all that we want in most cases.
\end{itemize}

With this in mind we should now find the most simple / efficient way to minimise $\varepsilon$ with respect to each of these quantities independently. 















\subsection{$f$ The probability (or adu) distribution}
Let us first consider the case where $V=1$, or when we only have a single random variable at each pixel. The derivative of the error with respect to $f$ is:
\begin{align}
   \frac{\partial \varepsilon(\mu, f)}{\partial f_j} &= -\frac{\partial}{\partial f_j} \sum_m^M \sum_i^I h^m_i \ln(f(g_m(i - \mu_m))) \\
   &= -\frac{\partial}{\partial f_j} \sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \ln(f_i) \\
   &= -\sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \frac{\delta_{i-j}}{f_i} \\
   &= -\frac{\sum_m^M h^m_{j/g_m+\mu_m}}{f_j}
\end{align}

In the present case this gradient vector will simply encourage ever increasing values of $f$. That is, we have to keep $f$ normalised. Now we could do this by representing $f$ in Fourier space and recalculating the gradient vector there (as we did for $\mu$). But then we also have the problem that values of $f$ for which $\sum_m^M h^m_{i/g_m+\mu_m}$ is zero have no effect on the error metric. Thus they are completely free to float in order to satisfy any normalisation constraint.

Let's choose one of the $f$ values to keep $f$ normalised:
\begin{align}
   f_j = \delta_{j} (1 - \sum_{i=1}^I f_i) + \sum_{i=1}^I \delta_{i-j} f_i
\end{align}

This means that not all of the $f$ are linearly independ, so we must recalculate the derivative term:
\begin{align}
   \frac{\partial \varepsilon(\mu, f)}{\partial f_j} &= -\frac{\partial}{\partial f_j} \sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \ln(f_i) \\
   &= - \sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \frac{1}{f_i} \frac{\partial f_i}{\partial f_j}
\end{align}

and
\begin{align}
   \frac{\partial f_i}{\partial f_j} &= \frac{\partial }{\partial f_j} \left[ \delta_{i} (1 - \sum_{k=1}^I f_k) + \sum_{k=1}^I \delta_{k-i} f_k \right] \quad \text{for } j > 0 \\
   &= -\delta_{i} + \delta_{j-i}
\end{align}

\begin{align}
   \frac{\partial \varepsilon(\mu, f)}{\partial f_j} &=- \sum_m^M \sum_i^I h^m_{i+\mu_m} \frac{1}{f_i} (-\delta_{i} + \delta_{j-i}) \\
   &= \frac{ \sum_m^M h^m_{(0+\mu_m)/g_m}}{f_0} - \frac{ \sum_m^M h^m_{(j+\mu_m)/g_m}}{f_j} \\
   &= \frac{ h_{0}}{f_0} - \frac{ h_{j}}{f_j}
\end{align}
where I have defined $h_j \equiv \sum_m^M h^m_{j/g_m+\mu_m}$. Notice that at this point our choice of normalisation has biased the evaluation of the gradient vector. For example, what if $h_0$ where zero? Now let's see if we can demand that $\partial \varepsilon(\mu, f) / \partial f_j = 0$:
\begin{align}
   \frac{ h_{0}}{f_0} &= \frac{ h_{j}}{f_j} \\
   \therefore f_j &= \frac{f_0}{h_0} h_j \\
   \therefore f_0 &= 1 -  \frac{f_0}{h_0} \sum_{j=1} h_j \\
   \therefore f_0 &= \frac{h_0}{h_0 + \sum_{j=1} h_j} = \frac{h_0}{\sum_{j=0} h_j} \\
   \therefore f_j &= \frac{h_0}{\sum_{j=0} h_j} \frac{1}{h_0} h_j = \frac{h_j}{\sum_{j=0} h_j}
\end{align}
Magic! Now our choice of $f_0$ as the normalisation term has no effect on the $f$'s chosen to minimise the log likelihood error. So, for any given estimate of $\mu$'s we can immediately maximise the likelihood. 

\subsubsection{$X$ The probability (or adu) distributions}
Now suppose that we have $V$ random variables. In this case we should let the relative amplitude of the distributions vary (because we don't have the same number of photons at each pixel). Our log likelihood error is then:
\begin{align}
   \varepsilon &= -\sum_m^M \sum_i^I h^m_i \ln(f^m(g_m(i - \mu_m))) = -\sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \ln(f^m_i) \\
   &= -\sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \ln\left(\sum_v n^m_v X^v_i\right)
\end{align}
were $X^v_i$ is the distribution for the $v$'th random variable at count or adu or $x$ value $i$ and $n^m_v$ is the fractional counts for the $v$'th random variable at pixel $m$. 

Let's begin by minimising $\varepsilon$ with respect to $X^u_i$, assuming that we know everything else:
\begin{align}
   \frac{\partial \varepsilon}{\partial X^u_j} &= -\sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \frac{1}{f^m_i} \frac{\partial f^m_i}{\partial X^u_j} \\
   &= - \sum_m^M \sum_i^I h^m_{i/g_m+\mu_m} \frac{1}{f^m_i}n^m_u(-\delta_{i} + \delta_{j-i}) \\
   &=  \sum_m^M n^m_u \frac{ h^m_{0/g_m+\mu_m}}{f^m_0}- \sum_m^M n^m_u \frac{ h^m_{j/g_m+\mu_m}}{f^m_j} 
   \label{d_twovar}
\end{align}
where :
\begin{align}
   \frac{\partial f^m_i}{\partial X^u_j} &= \frac{\partial }{\partial X^u_j} \left[ \sum_v n^m_v (1- \sum_{i=1} X^v_i), \sum_v n^m_v X^v_1, \cdots \right]\\
   &= \frac{\partial }{\partial X^u_j} \left[ n^m_u \delta_{i} (1 - \sum_{k=1}^I X^u_k) + n^m_u\sum_{k=1}^I \delta_{k-i} X^u_k + \cdots \right] \\
   &= n^m_u(-\delta_{i} + \delta_{j-i}) 
\end{align}

Ok so Eq. (\ref{d_twovar}) looks a little tricky. Let us suppose that the solution to Eq. (\ref{d_twovar}) is unique. In this case any solution that we come up with is the sulution. At first glance we might be tempted to make both terms zero (i.e. set $f^m_j \rightarrow \infty$) but then of course $f$ is not normalised. What if $f^m_j$ was independent of $m$? How about we set $f^m_j$ equal to it's average in $m$. That is, maybe the solution to Eq. (\ref{d_twovar}) only cares about the average of $f$ on $m$ and not about local fluctuations from pixel to pixel:
\begin{align}
   f^m_j &\rightarrow f_j = \frac{1}{M}\sum_m \left[ \sum_v n^m_v X^v_i \right] 
\end{align}
I possit that this might be the solution:
\begin{align}
   f_j &= \frac{\sum_m n^m_u h^m_{j/g_m+\mu_m}}{\sum_{j}\sum_m n^m_u h^m_{j/g_m+\mu_m}}
\end{align}

Substituting into Eq. (\ref{d_twovar}):
\begin{align}
   \frac{\partial \varepsilon}{\partial X^u_j} &= \sum_m^M n^m_u \frac{ h^m_{0/g_m+\mu_m}}{f^m_0}- \sum_m^M n^m_u \frac{ h^m_{j/g_m+\mu_m}}{f^m_j} \\
   &=  \left(\sum_m^M n^m_u h^m_{0/g_m+\mu_m}\right) \frac{\sum_{j}\sum_m n^m_u h^m_{j/g_m+\mu_m}}{\sum_m n^m_u h^m_{0/g_m+\mu_m}} \\
   & - \left(\sum_m^M n^m_u h^m_{j/g_m+\mu_m}\right) \frac{\sum_{j}\sum_m n^m_u h^m_{j/g_m+\mu_m}}{\sum_m n^m_u h^m_{j/g_m+\mu_m}} \\
   &= \sum_{j}\sum_m n^m_u h^m_{j/g_m+\mu_m} - \sum_{j}\sum_m n^m_u h^m_{j/g_m+\mu_m} \\
   &= 0
\end{align}
More magic! Now we have our answer:
\begin{align}
   \frac{1}{M}\sum_m \left[ \sum_v n^m_v X^v_j \right] &= \frac{\sum_m n^m_u h^m_{j/g_m+\mu_m}}{\sum_{j}\sum_m n^m_u h^m_{j/g_m+\mu_m}}
\end{align}
therefore:
\begin{align}
   X^u_j &= \frac{1}{\sum_m n^m_u}\left[M\frac{\sum_m n^m_u h^m_{j/g_m+\mu_m}}{\sum_{j}\sum_m n^m_u h^m_{j/g_m+\mu_m}} - \sum_{v\neq u} (\sum_m n^m_v) X^v_j \right] 
\end{align}
Looking at this, it seems to take a weighted sum of the histograms, normalises and then subtracts the expected value of the remainding distributions. This is then scaled by the expected amplitude of our distribution, then bam, solution.















\subsection{$\mu$ and $g$ The offset's (or dark values) and inverse gain}
We could refine the offsets ($\mu$'s) and gains ($g$'s) independently. But they are highly coupled, both in our model and probably physically. Thus it would be extreamly inefficient to update them separately.

The simplest brute force approach is this; Manually evaluate:
\begin{align}
   \argmax_{\mu_m, g_m}\left[ \sum_i^I h^m_i \ln(f^m(g_m (i - \mu_m))) \right]
\end{align}  
Bare with me, this is not as bad as it looks. First we can use the magic of fast Fourier transforms to evaluate:
\begin{align}
   \varepsilon(g_m) &= -\max\left[ \sum_i^I h^m_i \ln(f^m(g_m (i - \mu_m))) \right] \\
   &= -\max\left[ h^m_i \otimes \ln(f^m(g_m i)) \right] 
\end{align}  
where $\otimes$ is almost a convolution. If $h$ and $f$ are real (which they are) then:
\begin{align}
   \varepsilon(g_m) &= -\max\mathcal{F}^{-1}\left[ \hat{h}^m_i \times \mathcal{F}[\ln(f^m(g_m i))]^* \right] 
\end{align}  
so it is actually the cross-correlation of $h$ and $f$ (when $h$ and $f$ are real). Once $g_m$ has been solved by minimising the above equation. Then we can find $\mu_m$:
\begin{align}
   \mu_m &= \argmax\mathcal{F}^{-1}\left[ \hat{h}^m_i \times \mathcal{F}[\ln(f^m(g_m i))]^* \right]
\end{align}  
We found that fitting a quadratic to the above equation for $i = i_{i_0-1}, i_{i_0}, i_{i_0+1}$ where $i_0$ is the solution, then fitting the maximum of that provides good results. 
So, once the optimal $g_m$ is found, then $\mu_m$ is an automatic result. Note that simple linear interpolation, followed by normalisation, can be used to evaluate $f^m(g_m i)$.  

After updating each of the $\mu_m$ and $g_m$ values we should enforce $\sum_m \mu_m = 0$ and $\frac{1}{M}\sum_m g_m = 1$. This is because there is a degree of freedom lurking in the shadows... For example, we could make the substitution $f(i - c) \leftrightarrow \mu + c$ and be none the wiser! We could also make the substitution $f(c g_m i) \leftrightarrow g_m / c$, how would we look then! Like fools no doubt. We would have forgotten the faces of our fathers. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
\subsection{$N$ The counts for each random variable}
We want to minimise:
\begin{align}
   \varepsilon_m &= -\sum_i^I h^m_i \ln(f^m(g_mi - \mu_m)) = -\sum_i^I h^m_{(i+\mu_m)/g_m} \ln(f^m_i) \\
   &= - \sum_i^I h^m_{(i+\mu_m)/g_m} \ln\left(\sum_v n^m_v X^v_i\right)
\end{align}
with respect to the set of $n^m_v$'s. The counts for each random variable at each pixel ($N^m_v$) then follow from $N^m_v = (\sum_i h^m_i) \times n^m_v$.

Once again the values for $n^m_v$ are highly coupled. If we increase the value of one random variable we must decrease the rest. Such that:
\begin{align}
   \sum_v n^m_v &= 1  \quad \text{for all } m 
\end{align}

So, we should represent the $n^m_v$'s in a basis for which this is always true for each pixel. This can be achieved in Fourier space, where:
\begin{align} 
   n^m_v &= \frac{1}{V} \sum_{u=0}^{V-1} \hat{n}^m_u e^{2\pi i \frac{u v}{V}}, &&\\
   &= \frac{1}{V} \sum_{f_u=-V/2+1}^{V/2} \hat{\mu}_{f_u} e^{2\pi i v w_u}       &&\text{for even } V \\
   &= \frac{1}{V} \sum_{f_u=-(V+1)/2}^{(V+1)/2} \hat{\mu}_{f_u} e^{2\pi i v w_u} &&\text{for odd } V
\end{align}

Since $n$ is real the reciprocal space representation has Hermitian symmetry. That is, the negative Fourier frequencies are the complex conjugate of the positive Fourier frequencies. In the discrete Fourier transform one can interpret the first element as the zero frequency of $n$ (which is useful because we can demand that this is one), the first half of the array as the positive frequencies and the second half of the array as the negative frequencies. Because of this it is useful switch between the array index, $v$ in this case, and the Fourier index, $f_v$. Thankfully Python already has a function for doing this:

\begin{verbatim}
 V = 8
 l   0  1  2  3  4  5  6  7    range(V)
 fl  0  1  2  3  4 -3 -2 -1    np.fft.fftfreq(V, d=1/float(V))
 wl (0  1  2  3  4 -3 -2 -1)/V np.fft.fftfreq(V)
 
 V = 7
 l   0  1  2  3  4  5  6    range(V)
 fl  0  1  2  3 -3 -2 -1    np.fft.fftfreq(V, d=1/float(V))
 wl (0  1  2  3 -3 -2 -1)/V np.fft.fftfreq(V)
\end{verbatim}

So now, our independent values for $n$ are:
\begin{align}
   \hat{n}^m_u \text{  for } l &= 1, 2, 3 \cdots V/2 + 1 &&\text{V even}\\
   \hat{n}^m_u \text{  for } l &= 1, 2, 3 \cdots (V+1)/2 &&\text{V odd}\\
\text{where  }   \hat{n}^m_0 &= 1   && \\
                 \hat{n}^m_{l_v} &= \hat{n}^{m*}_{-l_v} &&
\end{align}

Ok, so let's go back to our pixelwise error:
\begin{align}
   \varepsilon_m &= - \sum_i^I h^m_{(i+\mu_m)/g_m} \ln\left(\sum_v n^m_v X^v_i\right)
\end{align}
and evaluate $\sum_v n^m_v X^v_i$:
\begin{align}
   \sum_v n^m_v X^v_i &= \sum_{v=0}^{V-1} \left( \frac{1}{V} \sum_{u=0}^{V-1} \hat{n}^m_u e^{2\pi i \frac{u v}{V}} \right) X^v_i \\
   &= \sum_{u=0}^{V-1} \hat{n}^m_u  \left( \frac{1}{V}  \sum_{v=0}^{V-1}  X^v_i e^{2\pi i \frac{u v}{V}}\right) \\
   &= \sum_{u=0}^{V-1} \hat{n}^m_u  Y^u_i \\
   &= Y^0_i + \sum_{f_u=1}^{V_h} \hat{n}^m_{f_u} Y^{f_u}_i + \left(\sum_{V_l}^{f_u=-1} \hat{n}^m_{f_u} Y^{f_u}_i\right) 
\end{align}
where I have used $V_l$ and $V_h$ to represent the limits of the sum for both odd and even $V$. The vector inside the brackets is related by Hermitian symmetry to the vector $\sum_{f_u=1}^{V_h} \hat{n}^m_{f_u} Y^{f_u}_i$.

We can evaluate the gradient vector with the help of:
\begin{align*}
   \frac{\partial}{\partial \Re\{ \hat{n}^m_u \}} \sum_v n^m_v X^v_i &= \frac{\partial}{\partial \Re\{ \hat{n}^m_u \}}\left[Y^0_i + \sum_{f_v=1}^{V_h} \hat{n}^m_{f_v} Y^{f_v}_i + \left(\sum_{V_l}^{f_v=-1} \hat{n}^m_{f_v} Y^{f_v}_i\right) \right] \\
   &= 2 \Re\{Y^{u}_i\} \quad \text{for odd V} 
\end{align*}
I will ommit the imaginary component and cut to the chase:
\begin{align*}
   \frac{\partial}{\partial \hat{n}^m_u} \sum_v n^m_v X^v_i &\equiv \left[\frac{\partial}{\partial \Re\{ \hat{n}^m_u \}} + i \frac{\partial}{\partial \Im\{ \hat{n}^m_u \}}\right]  \sum_v n^m_v X^v_i \\
   &= 2 Y^{u}_i \quad \text{for odd V} 
\end{align*}
And... drum roll please:
\begin{align*}
   \frac{\partial \varepsilon_m}{\partial \hat{n}^m_u}  &= -\frac{\partial}{\partial \hat{n}^m_u} \sum_i^I h^m_{(i+\mu_m)/g_m} \ln\left(\sum_v n^m_v X^v_i\right) \\
   &= - \sum_i^I \frac{h^m_{(i+\mu_m)/g_m}}{f^m_i} \frac{\partial}{\partial \hat{n}^m_u} \sum_v n^m_v X^v_i \\
   &= - 2 \sum_i^I \frac{h^m_{(i+\mu_m)/g_m}}{f^m_i} Y^{u}_i \quad \text{for odd V} 
\end{align*}


When $V$ is even there is an annoying extra (real) positive Fourier component when evaluating $\frac{\partial f^m_i}{\partial \hat{n}^m_u}$ leading to:
\begin{align*}
   \frac{\partial \varepsilon_m}{\partial \hat{n}^m_u} &= - 2 \sum_i^I \frac{h^m_{(i+\mu_m)/g_m}}{f^m_i} Y^{u}_i && \text{for even V and } u \neq V/2 \\
   &= - \sum_i^I \frac{h^m_{(i+\mu_m)/g_m}}{f^m_i} \Re\{Y^{u}_i\} &&\text{for even V and } u = V/2 
\end{align*}
actually $Y^{V/2}_i$ is automatically real when V is even, but I just wanted to be sure that you know that $\Im\{ \hat{n}^m_{V/2}\}$ is not a variable in this case.

For both odd and even V the error is:
\begin{align*}
   \varepsilon_m &= - \sum_i^I h^m_{(i+\mu_m)/g_m} \ln\left(\sum_v n^m_v X^v_i\right) \\
   &= - \sum_i^I h^m_{(i+\mu_m)/g_m} \ln\left(Y^0_i + 2\Re\left\{\sum_{f_u=1}^{V_h} \hat{n}^m_{f_u} Y^{f_u}_i\right\}\right)
\end{align*}





\end{document}


